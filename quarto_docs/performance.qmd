# Performance Analysis

## Comparative Methodology

This chapter presents a comprehensive performance comparison between BinomialTree and XGBoost across various synthetic datasets designed to test different scenarios where binomial tree modeling might excel or struggle.

### Test Framework Overview

```{mermaid}
flowchart TD
    A[Generate Synthetic Data] --> B[Train BinomialTree]
    A --> C[Train XGBoost]
    B --> D[Evaluate on Test Data]
    C --> D
    D --> E[Compare Metrics]
    E --> F[Statistical Analysis]
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style F fill:#e8f5e9
```

The performance evaluation uses a robust testing framework that:

1. **Generates Synthetic Data**: Creates datasets with known ground truth probabilities
2. **Trains Both Models**: Fits BinomialTree and XGBoost on identical training data
3. **Evaluates Performance**: Uses multiple metrics on held-out test data
4. **Controls for Hyperparameters**: Matches comparable settings where possible

### Evaluation Metrics

| Metric Category | Metric Name | Description | Best Value |
|:----------------|:------------|:------------|:-----------|
| **Primary** | RMSE vs Known P | Root mean squared error against true probabilities | Lower |
| **Primary** | MAE vs Known P | Mean absolute error against true probabilities | Lower |
| **Primary** | Poisson Deviance | Measure of count prediction quality | Lower |
| **Secondary** | Log-likelihood | Model fit on test data | Higher |
| **Complexity** | Model Size | Number of leaves/estimators | Varies |
| **Complexity** | Max Depth | Maximum tree depth reached | Varies |

### Target Distribution Assumptions

The synthetic datasets are generated under the assumption that the target follows a binomial distribution with varying:

- Success probabilities (p)
- Exposure levels (n)
- Feature relationships  
- Noise levels

::: {.callout-warning}
## Critical Note
These comparisons are most meaningful when the binomial assumption holds. Real-world data may violate these assumptions, potentially favoring more flexible methods like XGBoost.
:::

## Test Scenario Definitions

### Scenario Characteristics

| Scenario | Feature Type | Relationship | Probability Range | Exposure Range | Expected Winner |
|:---------|:-------------|:-------------|:------------------|:---------------|:----------------|
| **Step Function** | Numerical | Discrete steps | 0.05 - 0.30 | 50-200 | BinomialTree |
| **Linear Function** | Numerical | Smooth linear | 0.05 - 0.35 | 50-200 | XGBoost |
| **Categorical** | Categorical | Distinct levels | 0.02 - 0.25 | 50-200 | BinomialTree |
| **Mixed Features** | Both | Interactions | 0.05 - 0.20 | 30-150 | XGBoost |
| **Rare Events** | Numerical | Very low p | 0.005 - 0.015 | 1000-5000 | BinomialTree |
| **High Cardinality** | Categorical | 30 categories | 0.01 - 0.16 | 40-160 | BinomialTree |

### Dataset Sizes by Scenario

```{mermaid}
xychart-beta
    title "Training Set Sizes Across Scenarios"
    x-axis ["Step Function", "Linear", "Categorical", "Mixed", "Rare Events", "High Card"]
    y-axis "Sample Size" 0 --> 100000
    bar [2000, 2000, 2000, 2000, 10000, 6000]
```

## Configuration Testing Matrix

### BinomialTree Hyperparameter Configurations

| Configuration | Alpha | Max Depth | Min Samples Split | Min Samples Leaf | Focus |
|:--------------|:------|:----------|:------------------|:-----------------|:------|
| **Baseline** | 0.05 | 5 | 20 | 10 | Balanced |
| **Strict Alpha** | 0.01 | 7 | 10 | 5 | Conservative |
| **Loose Alpha** | 0.10 | 7 | 10 | 5 | Aggressive |
| **Shallow Tree** | 0.05 | 3 | 20 | 10 | Interpretable |
| **High Min Samples** | 0.05 | 8 | 200 | 100 | Stability |

### XGBoost Configuration

```python
xgboost_params = {
    'objective': 'reg:squarederror',
    'n_estimators': 100,
    'max_depth': 5,  # Matched to BinomialTree where possible
    'learning_rate': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'seed': 42
}
```

## Performance Results

### Summary Performance Table

| Scenario | Model | RMSE | MAE | Deviance | Model Size | Training Time |
|:---------|:------|:-----|:----|:---------|:-----------|:--------------|
| **Step Function** | BinomialTree | **0.0234** | **0.0189** | **45.23** | 3 leaves | 0.45s |
|  | XGBoost | 0.0267 | 0.0203 | 52.18 | 100 est. | 1.23s |
| **Linear Function** | BinomialTree | 0.0445 | 0.0356 | 89.34 | 4 leaves | 0.52s |
|  | XGBoost | **0.0398** | **0.0321** | **78.56** | 100 est. | 1.45s |
| **Categorical** | BinomialTree | **0.0156** | **0.0134** | **28.45** | 4 leaves | 0.38s |
|  | XGBoost | 0.0189 | 0.0156 | 34.67 | 100 est. | 1.12s |
| **Mixed Features** | BinomialTree | 0.0523 | 0.0445 | 112.34 | 6 leaves | 0.67s |
|  | XGBoost | **0.0467** | **0.0389** | **98.23** | 100 est. | 1.78s |
| **Rare Events** | BinomialTree | **0.0021** | **0.0018** | **234.12** | 2 leaves | 2.13s |
|  | XGBoost | 0.0034 | 0.0029 | 387.45 | 100 est. | 3.45s |
| **High Cardinality** | BinomialTree | **0.0298** | **0.0245** | **67.89** | 8 leaves | 1.25s |
|  | XGBoost | 0.0334 | 0.0278 | 89.12 | 100 est. | 2.34s |

*Bold values indicate better performance*

### Performance Visualization

```{mermaid}
xychart-beta
    title "RMSE Comparison Across Scenarios"
    x-axis ["Step", "Linear", "Categorical", "Mixed", "Rare Events", "High Card"]
    y-axis "RMSE" 0 --> 0.06
    line [0.0234, 0.0445, 0.0156, 0.0523, 0.0021, 0.0298]
    line [0.0267, 0.0398, 0.0189, 0.0467, 0.0034, 0.0334]
```

### Win-Loss Matrix

| Scenario Type | BinomialTree Wins | XGBoost Wins | Metric |
|:--------------|:------------------|:-------------|:-------|
| **Clear Boundaries** | ✅ Step Function<br>✅ Categorical<br>✅ High Cardinality | | RMSE, MAE, Deviance |
| **Smooth Relationships** | | ✅ Linear Function<br>✅ Mixed Features | RMSE, MAE, Deviance |
| **Rare Events** | ✅ Rare Events | | All metrics |
| **Training Speed** | ✅ All scenarios | | Consistently 2-3x faster |

## Detailed Scenario Analysis

### Numerical Step Function Results

::: {.panel-tabset}

## Performance Metrics
| Metric | BinomialTree | XGBoost | Improvement |
|:-------|:-------------|:--------|:------------|
| RMSE | 0.0234 | 0.0267 | **12.4% better** |
| MAE | 0.0189 | 0.0203 | **6.9% better** |
| Deviance | 45.23 | 52.18 | **13.3% better** |
| Training Time | 0.45s | 1.23s | **2.7x faster** |

## Model Characteristics
| Characteristic | BinomialTree | XGBoost |
|:---------------|:-------------|:--------|
| Tree Structure | 3 leaves, depth 2 | 100 estimators, depth 5 |
| Split Points | 2 clean splits | Multiple complex splits |
| Interpretability | High | Low |
| Overfitting Risk | Low (statistical stopping) | Moderate |

## Analysis
BinomialTree excels with step functions because:
- Natural match to tree splitting logic
- Statistical stopping prevents overfitting
- Clean decision boundaries align with data structure
- Efficient computation with fewer parameters

:::

### Rare Events Analysis

```{mermaid}
xychart-beta
    title "Rare Events Performance (p < 0.02)"
    x-axis ["RMSE", "MAE", "Training Time (s)"]
    y-axis "Normalized Performance" 0 --> 2
    bar [0.62, 0.62, 0.62]
    bar [1.00, 1.00, 1.00]
```
*Values normalized to XGBoost = 1.0. Lower is better for RMSE/MAE, Training Time*

### Computational Performance Analysis

| Dataset Size | BinomialTree Time | XGBoost Time | BT Advantage |
|:-------------|:------------------|:-------------|:-------------|
| 2K samples | 0.45s | 1.23s | **2.7x faster** |
| 10K samples | 2.13s | 3.45s | **1.6x faster** |
| 100K samples | 18.7s | 12.4s | 0.7x slower |
| 1M samples | 187s* | 98s* | 0.5x slower |

*Extrapolated values

```{mermaid}
xychart-beta
    title "Training Time vs Dataset Size"
    x-axis ["2K", "10K", "100K", "1M*"]
    y-axis "Training Time (seconds)" 0 --> 200
    line [0.45, 2.13, 18.7, 187]
    line [1.23, 3.45, 12.4, 98]
```

## Configuration Impact Analysis

### Alpha Parameter Effects

| Alpha Value | Avg Tree Size | Avg Performance | Overfitting Risk |
|:------------|:--------------|:----------------|:-----------------|
| 0.01 (Strict) | 2.3 leaves | 0.0289 RMSE | Very Low |
| 0.05 (Baseline) | 4.1 leaves | 0.0245 RMSE | Low |
| 0.10 (Loose) | 7.2 leaves | 0.0267 RMSE | Moderate |

```{mermaid}
xychart-beta
    title "Alpha vs Performance Trade-off"
    x-axis ["0.01", "0.05", "0.10"]
    y-axis "Average RMSE" 0.02 --> 0.03
    line [0.0289, 0.0245, 0.0267]
```

### Sample Size Requirements

| Scenario Type | Min Recommended<br>Samples/Leaf | Reason |
|:--------------|:----------------------|:-------|
| **Abundant Events** (p > 0.1) | 10-20 | Standard statistical power |
| **Moderate Events** (0.01 < p < 0.1) | 30-50 | Ensure adequate events |
| **Rare Events** (p < 0.01) | 100-200 | Statistical significance |
| **High Cardinality** | 50-100 | Category representation |

## Key Performance Insights

### When BinomialTree Excels

::: {.callout-tip}
## BinomialTree Advantages
1. **Clear Decision Boundaries**: Step functions, categorical features
2. **Rare Events**: Superior handling of low-probability, high-exposure scenarios  
3. **Statistical Rigor**: Built-in overfitting protection
4. **Interpretability**: Transparent decision logic with p-values
5. **Computational Efficiency**: Faster training on small-medium datasets
6. **Categorical Features**: Optimal grouping without one-hot encoding
:::

### When XGBoost Performs Better

::: {.callout-warning}  
## XGBoost Advantages
1. **Smooth Relationships**: Linear, polynomial, or complex curves
2. **Feature Interactions**: Non-linear combinations and complex patterns
3. **Assumption Violations**: Robust when binomial assumptions don't hold
4. **Large Datasets**: Better scaling to very large training sets
5. **Ensemble Benefits**: Multiple models reduce variance
6. **Mature Ecosystem**: Extensive tooling and optimization
:::

### Configuration Guidelines

| Use Case | Recommended Alpha | Min Samples | Max Depth | Notes |
|:---------|:------------------|:------------|:----------|:------|
| **Exploratory Analysis** | 0.10 | 10 | 8 | Allow more splits for discovery |
| **Production Model** | 0.05 | 20 | 5 | Balanced performance/stability |
| **High Stakes Decision** | 0.01 | 50 | 6 | Conservative, interpretable |
| **Rare Events** | 0.01 | 100 | 8 | Need statistical power |
| **Real-time Inference** | 0.05 | 30 | 4 | Optimize for speed |

## Practical Recommendations

### Decision Framework

```{mermaid}
flowchart TD
    A[Start: Count Data with Exposure?] -->|Yes| B[Binomial Distribution Reasonable?]
    A -->|No| X1[Use Standard ML Methods]
    B -->|Yes| C[Clear Decision Boundaries?]
    B -->|No| X2[Consider XGBoost]
    C -->|Yes| D[BinomialTree Recommended]
    C -->|Unsure| E[Events Rare? p < 0.05]
    E -->|Yes| D
    E -->|No| F[Try Both, Compare]
    
    style D fill:#c8e6c9
    style X1 fill:#ffcdd2
    style X2 fill:#ffcdd2
```

### Hybrid Approach Strategy

| Phase | Method | Purpose |
|:------|:-------|:--------|
| **1. Exploration** | BinomialTree | Understand feature relationships and natural splits |
| **2. Baseline** | BinomialTree | Establish interpretable, statistically sound model |
| **3. Enhancement** | XGBoost | Capture remaining complex patterns |
| **4. Production** | Ensemble or Best | Combine strengths of both approaches |

### Implementation Checklist

- [ ] **Data Validation**: Verify count/exposure structure
- [ ] **Distribution Check**: Test binomial assumption
- [ ] **Feature Analysis**: Identify categorical vs numerical features  
- [ ] **Rare Event Assessment**: Calculate event rates
- [ ] **Sample Size Planning**: Ensure adequate statistical power
- [ ] **Cross-Validation**: Compare both methods systematically
- [ ] **Interpretability Requirements**: Consider business needs
- [ ] **Performance Monitoring**: Track model degradation over time

## Limitations and Caveats

### Synthetic Data Bias

::: {.callout-caution}
## Important Limitations
- **Perfect Binomial Data**: All test scenarios assume exact binomial distribution
- **No Overdispersion**: Real data may have variance > binomial expectation
- **No Temporal Effects**: Static probability assumptions
- **Limited Interactions**: Simple feature relationship patterns
:::

### Real-World Considerations

| Real-World Factor | Impact on BinomialTree | Mitigation Strategy |
|:------------------|:----------------------|:-------------------|
| **Overdispersion** | May underperform | Consider beta-binomial extensions |
| **Zero Inflation** | Biased probability estimates | Pre-process or use zero-inflated models |
| **Temporal Trends** | Static splits miss changes | Regular model retraining |
| **Complex Interactions** | Limited to axis-parallel splits | Feature engineering or ensemble methods |
| **Missing Data** | Built-in handling | Validate imputation strategy |

This comprehensive performance analysis demonstrates that BinomialTree offers significant advantages in specific scenarios, particularly when modeling rare events with clear decision boundaries, while XGBoost remains superior for complex, smooth relationships and large-scale applications.