[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "Overview",
    "text": "Overview\nBinomialTree is a specialized decision tree algorithm designed specifically for modeling rare count events with exposure values. Unlike traditional decision trees that split on variance reduction or information gain, BinomialTree optimizes splits by maximizing the binomial likelihood, making it particularly suited for scenarios where:\n\nThe target is a count variable (k) with known exposure (n)\nEvents are relatively rare (low success probability)\nThe goal is to predict the probability of event occurrence\nThe underlying data follows or approximates a binomial distribution",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "Key Features",
    "text": "Key Features\n\nBinomial Likelihood Maximization: Tree splits are chosen to maximize the binomial log-likelihood rather than traditional metrics\nStatistical Stopping Criteria: Uses hypothesis testing with Bonferroni correction to determine when to stop splitting, inspired by Conditional Inference Trees (ctree)\nExposure-Aware: Designed to handle count data with varying exposure levels\nRobust Splitting: Handles both numerical and categorical features with appropriate split strategies",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-binomialtree",
    "href": "index.html#why-binomialtree",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "Why BinomialTree?",
    "text": "Why BinomialTree?\nTraditional decision trees and ensemble methods like Random Forest or XGBoost are optimized for minimizing squared error or other general loss functions. However, when modeling rare events:\n\nCount/Exposure Structure: The natural structure of count data with exposure (k successes out of n trials) is better captured by binomial likelihood\nProbability Distribution: For rare events, modeling count/n directly can be problematic due to the poorly behaved probability distribution of ratios\nStatistical Rigor: The hypothesis testing framework reduces overfitting and may reduce the need for train-test splits",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#when-to-use-binomialtree",
    "href": "index.html#when-to-use-binomialtree",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "When to Use BinomialTree",
    "text": "When to Use BinomialTree\nBinomialTree performs best when:\n\nYour target follows or approximates a binomial distribution\nYou have count data with exposure information\nEvents are relatively rare (p &lt; 0.5, ideally p &lt; 0.1)\nYou want interpretable decision boundaries\nStatistical stopping criteria are important for your use case\n\nNote: If the binomial distribution assumptions are significantly violated, traditional gradient boosting methods like XGBoost may perform better.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#about-this-documentation",
    "href": "index.html#about-this-documentation",
    "title": "BinomialTree: A Binomial Decision Tree for Rare Event Modeling",
    "section": "About This Documentation",
    "text": "About This Documentation\nThis documentation is organized into three main sections:\n\nTheory: Mathematical foundations, splitting criteria, and stopping conditions\nImplementation: Practical usage guide and API reference\n\nPerformance: Comparative analysis with XGBoost on various datasets\n\nThe examples and comparisons in this documentation demonstrate both the strengths and limitations of the binomial tree approach.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "theory.html",
    "href": "theory.html",
    "title": "1  Mathematical Foundations",
    "section": "",
    "text": "1.1 Binomial Distribution Foundation\nThe BinomialTree algorithm is built on the assumption that the target data follows a binomial distribution. For each observation \\(i\\), we have:\nThe likelihood for observation \\(i\\) under a binomial distribution is:\n\\[P(k_i | n_i, p) = \\binom{n_i}{k_i} p^{k_i} (1-p)^{n_i - k_i}\\]\nThe log-likelihood for observation \\(i\\) is:\n\\[\\ell_i(p) = \\log\\binom{n_i}{k_i} + k_i \\log(p) + (n_i - k_i) \\log(1-p)\\]\nFor a set of observations in a node, the total log-likelihood is:\n\\[\\ell_{node}(p) = \\sum_{i \\in node} \\ell_i(p)\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#binomial-distribution-foundation",
    "href": "theory.html#binomial-distribution-foundation",
    "title": "1  Mathematical Foundations",
    "section": "",
    "text": "\\(k_i\\): number of successes (events)\n\\(n_i\\): number of trials (exposure)\n\\(p_i\\): true probability of success (unknown, to be estimated)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#why-model-counts-instead-of-rates",
    "href": "theory.html#why-model-counts-instead-of-rates",
    "title": "1  Mathematical Foundations",
    "section": "1.2 Why Model Counts Instead of Rates?",
    "text": "1.2 Why Model Counts Instead of Rates?\nA key insight is that we model the count data directly rather than the rate \\(k_i/n_i\\). This is motivated by several factors:\n\n1.2.1 Statistical Properties\nFor rare events where \\(p\\) is small and \\(n_i\\) varies significantly:\n\nVariance Instability: The variance of \\(k_i/n_i\\) is \\(p(1-p)/n_i\\), which depends on \\(n_i\\)\nDistribution Shape: The distribution of \\(k_i/n_i\\) becomes highly skewed and poorly behaved for small \\(p\\) and varying \\(n_i\\)\nInformation Loss: Converting to rates discards the exposure information that affects uncertainty\n\n\n\n1.2.2 Practical Implications\nConsider two observations: - Observation A: 1 success out of 10 trials (\\(k/n = 0.1\\))\n- Observation B: 10 successes out of 100 trials (\\(k/n = 0.1\\))\nBoth have the same rate but vastly different uncertainty. The binomial likelihood naturally accounts for this difference through the exposure term.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#splitting-criteria",
    "href": "theory.html#splitting-criteria",
    "title": "1  Mathematical Foundations",
    "section": "1.3 Splitting Criteria",
    "text": "1.3 Splitting Criteria\n\n1.3.1 Objective Function\nAt each node, we seek the split that maximizes the combined log-likelihood of the resulting child nodes:\n\\[\\text{Split Quality} = \\ell_{left}(\\hat{p}_{left}) + \\ell_{right}(\\hat{p}_{right}) - \\ell_{parent}(\\hat{p}_{parent})\\]\nWhere \\(\\hat{p}\\) is the maximum likelihood estimate: \\(\\hat{p} = \\frac{\\sum k_i}{\\sum n_i}\\)\n\n\n1.3.2 Numerical Features\nFor numerical features, we:\n\nSort observations by feature value\nConsider all possible split points (midpoints between consecutive unique values)\nFor computational efficiency, subsample split points if there are more than max_numerical_split_points unique values\nHandle missing values by always assigning them to the right child\n\nThe algorithm efficiently computes split statistics by maintaining cumulative sums as it iterates through sorted values.\n\n\n1.3.3 Categorical Features\nFor categorical features with \\(C\\) categories, we use an optimal grouping strategy:\n\nSort by Rate: Order categories by their estimated success rate \\(\\hat{p}_c = \\frac{\\sum_{i \\in c} k_i}{\\sum_{i \\in c} n_i}\\)\nOptimal Grouping: Consider all possible ways to split the sorted categories into two groups\nDegrees of Freedom: The split has \\(df = C - 1\\) degrees of freedom for hypothesis testing\n\nThis approach is theoretically optimal for binomial data and avoids the exponential complexity of considering all possible category groupings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#statistical-stopping-criteria",
    "href": "theory.html#statistical-stopping-criteria",
    "title": "1  Mathematical Foundations",
    "section": "1.4 Statistical Stopping Criteria",
    "text": "1.4 Statistical Stopping Criteria\n\n1.4.1 Motivation\nTraditional decision trees often require external validation or pruning to prevent overfitting. BinomialTree incorporates statistical hypothesis testing directly into the splitting process, inspired by Conditional Inference Trees (ctree).\n\n\n1.4.2 Hypothesis Testing Framework\nFor each potential split, we test:\n\nNull Hypothesis (\\(H_0\\)): The split provides no improvement over the parent node\nAlternative Hypothesis (\\(H_1\\)): The split provides significant improvement\n\n\n\n1.4.3 Likelihood Ratio Test\nThe test statistic is:\n\\[LR = 2(\\ell_{children} - \\ell_{parent})\\]\nUnder \\(H_0\\), this follows a chi-squared distribution with degrees of freedom: - Numerical features: \\(df = 1\\) - Categorical features: \\(df = C - 1\\) (where \\(C\\) is the number of categories)\n\n\n1.4.4 Bonferroni Correction\nSince we test multiple features at each node, we apply Bonferroni correction:\n\\[p_{adjusted} = \\min(1, p_{raw} \\times m)\\]\nWhere \\(m\\) is the number of features tested. The split is considered significant if \\(p_{adjusted} &lt; \\alpha\\).\n\n\n1.4.5 Implementation Details\nThe stopping procedure follows these steps:\n\nPre-split Checks: Verify minimum sample requirements and maximum depth\nFind Best Split: For each feature, find the split that maximizes log-likelihood gain\nCalculate P-values: Compute likelihood ratio test p-value for each feature’s best split\nMultiple Testing Correction: Apply Bonferroni correction to the minimum p-value\nDecision: Split only if adjusted p-value &lt; α",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#permutation-tests-future-extension",
    "href": "theory.html#permutation-tests-future-extension",
    "title": "1  Mathematical Foundations",
    "section": "1.5 Permutation Tests (Future Extension)",
    "text": "1.5 Permutation Tests (Future Extension)\nWhile the current implementation uses the likelihood ratio test with chi-squared distribution, the framework supports permutation tests for more robust inference:\n\n1.5.1 Procedure\n\nCompute observed test statistic for the best split\nRandomly permute the target values while keeping exposure fixed\nRecompute test statistic on permuted data\nRepeat B times to build null distribution\nP-value = proportion of permuted statistics ≥ observed statistic\n\n\n\n1.5.2 Advantages\n\nNo distributional assumptions\nExact finite-sample validity\nRobust to model misspecification\n\n\n\n1.5.3 Current Status\nThe permutation test framework is designed into the architecture but not yet implemented, as the likelihood ratio test provides good performance with much lower computational cost.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#assumptions-and-limitations",
    "href": "theory.html#assumptions-and-limitations",
    "title": "1  Mathematical Foundations",
    "section": "1.6 Assumptions and Limitations",
    "text": "1.6 Assumptions and Limitations\n\n1.6.1 Key Assumptions\n\nBinomial Distribution: Target data follows or approximates a binomial distribution\nIndependence: Observations are independent conditional on features\nConstant Probability: Within each leaf, the success probability is constant\n\n\n\n1.6.2 When Assumptions Are Violated\n\nOverdispersion: If variance &gt; mean (for count data), consider beta-binomial models\nTemporal Dependence: Time-varying probabilities may violate independence\nZero-Inflation: Excess zeros may indicate a different generating process\n\n\n\n1.6.3 Performance Implications\nBinomialTree should perform well when assumptions are met but may underperform gradient boosting methods when assumptions are significantly violated. The statistical stopping criteria help prevent overfitting but may also limit the model’s ability to capture complex nonlinear relationships.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "theory.html#theoretical-advantages",
    "href": "theory.html#theoretical-advantages",
    "title": "1  Mathematical Foundations",
    "section": "1.7 Theoretical Advantages",
    "text": "1.7 Theoretical Advantages\n\nPrincipled Splitting: Splits directly optimize the relevant likelihood rather than surrogate measures\nUncertainty Quantification: Naturally handles varying exposure levels\nStatistical Rigor: Hypothesis testing framework reduces overfitting\nInterpretability: Clear decision boundaries with statistical significance\nReduced Validation Needs: Statistical stopping may reduce the need for train-test splits\n\nThis theoretical foundation guides the practical implementation and helps understand when BinomialTree is likely to excel compared to traditional methods.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Mathematical Foundations</span>"
    ]
  },
  {
    "objectID": "implementation.html",
    "href": "implementation.html",
    "title": "2  Implementation Guide",
    "section": "",
    "text": "2.1 Installation and Dependencies\nBinomialTree is implemented in pure Python with minimal dependencies:\nNo external machine learning libraries are required for the core functionality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#installation-and-dependencies",
    "href": "implementation.html#installation-and-dependencies",
    "title": "2  Implementation Guide",
    "section": "",
    "text": "# Required dependencies\nimport numpy as np\nimport pandas as pd  # Optional, for DataFrame support",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#basic-usage",
    "href": "implementation.html#basic-usage",
    "title": "2  Implementation Guide",
    "section": "2.2 Basic Usage",
    "text": "2.2 Basic Usage\n\n2.2.1 Data Preparation\nYour data should contain: - Target column: Number of successes (k) - Exposure column: Number of trials (n) - Feature columns: Predictor variables (numerical or categorical)\n# Example data structure\ndata = [\n    {'feature_num': 10.0, 'feature_cat': 'A', 'successes': 2, 'trials': 20},\n    {'feature_num': 12.0, 'feature_cat': 'B', 'successes': 8, 'trials': 25},\n    {'feature_num': 15.0, 'feature_cat': 'A', 'successes': 3, 'trials': 18},\n    # ... more observations\n]\n\n# Or as a pandas DataFrame\nimport pandas as pd\ndf = pd.DataFrame(data)\n\n\n2.2.2 Basic Model Training\nfrom binomial_tree.tree import BinomialDecisionTree\n\n# Initialize the tree\ntree = BinomialDecisionTree(\n    min_samples_split=20,\n    min_samples_leaf=10, \n    max_depth=5,\n    alpha=0.05,\n    verbose=True\n)\n\n# Fit the model\ntree.fit(\n    data=data,  # or df for pandas DataFrame\n    target_column='successes',\n    exposure_column='trials', \n    feature_columns=['feature_num', 'feature_cat']\n)\n\n# Make predictions\nnew_data = [\n    {'feature_num': 13.0, 'feature_cat': 'A'},\n    {'feature_num': 23.0, 'feature_cat': 'C'}\n]\npredicted_probabilities = tree.predict_p(new_data)\n\n\n2.2.3 Inspecting the Model\n# Print the tree structure\ntree.print_tree()\n\n# Output example:\n# Split: feature_num &lt;= 15.500 (p-val=0.0123, gain=12.45) | k=45, n=180 (p̂=0.250)\n#   |--L: Split: feature_cat in {'A', 'C'} (p-val=0.0089, gain=8.32) | k=15, n=80 (p̂=0.188)\n#   |    |--L: Leaf: k=8, n=50 (p̂=0.160) | Reason: stat_stop\n#   |    +--R: Leaf: k=7, n=30 (p̂=0.233) | Reason: min_samples_split\n#   +--R: Leaf: k=30, n=100 (p̂=0.300) | Reason: stat_stop",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#hyperparameter-configuration",
    "href": "implementation.html#hyperparameter-configuration",
    "title": "2  Implementation Guide",
    "section": "2.3 Hyperparameter Configuration",
    "text": "2.3 Hyperparameter Configuration\n\n2.3.1 Core Parameters\ntree = BinomialDecisionTree(\n    # Structural constraints\n    max_depth=5,                    # Maximum tree depth\n    min_samples_split=20,           # Min samples to consider splitting\n    min_samples_leaf=10,            # Min samples in each leaf\n    \n    # Statistical stopping\n    alpha=0.05,                     # Significance level for splits\n    \n    # Performance tuning  \n    max_numerical_split_points=255, # Limit split points for large features\n    \n    # Output control\n    verbose=False,                  # Enable detailed logging\n    confidence_level=0.95           # For confidence intervals (display only)\n)\n\n\n2.3.2 Parameter Guidelines\nalpha (Significance Level) - Lower values (0.01) create more conservative, smaller trees - Higher values (0.10) allow more aggressive splitting - Default 0.05 provides good balance\nmin_samples_split and min_samples_leaf - Increase for rare events to ensure statistical power - Decrease for abundant data to capture fine patterns - Rule of thumb: min_samples_leaf ≥ 5-10 expected events\nmax_depth - Acts as a safety constraint - Statistical stopping often kicks in before max depth - Set higher when alpha is strict (low)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#advanced-usage",
    "href": "implementation.html#advanced-usage",
    "title": "2  Implementation Guide",
    "section": "2.4 Advanced Usage",
    "text": "2.4 Advanced Usage\n\n2.4.1 Feature Type Specification\n# Explicit feature type control\ntree.fit(\n    data=data,\n    target_column='successes',\n    exposure_column='trials',\n    feature_columns=['numeric_feat', 'categorical_feat'],\n    feature_types={\n        'numeric_feat': 'numerical',\n        'categorical_feat': 'categorical'\n    }\n)\n\n\n2.4.2 Missing Value Handling\nBinomialTree handles missing values automatically:\nNumerical Features - Missing values imputed with median during training - Same median used for prediction\nCategorical Features\n- Missing values treated as a distinct category (‘NaN’) - Unseen categories in prediction mapped to NaN path\n# Data with missing values\ndata_with_missing = [\n    {'num_feat': 10.0, 'cat_feat': 'A', 'k': 2, 'n': 20},\n    {'num_feat': None, 'cat_feat': 'B', 'k': 8, 'n': 25},  # Missing numeric\n    {'num_feat': 15.0, 'cat_feat': None, 'k': 3, 'n': 18}, # Missing categorical\n]\n\n# No special handling needed\ntree.fit(data=data_with_missing, ...)\n\n\n2.4.3 Pandas Integration\nimport pandas as pd\nimport numpy as np\n\n# Create DataFrame with missing values\ndf = pd.DataFrame({\n    'numeric_feature': [10.0, 12.0, np.nan, 15.0],\n    'categorical_feature': ['A', 'B', 'A', None], \n    'successes': [2, 8, 1, 3],\n    'trials': [20, 25, 5, 18]\n})\n\n# Seamless integration\ntree.fit(\n    data=df,\n    target_column='successes', \n    exposure_column='trials',\n    feature_columns=['numeric_feature', 'categorical_feature']\n)\n\n# Prediction on new DataFrame\nnew_df = pd.DataFrame({\n    'numeric_feature': [13.0, 23.0],\n    'categorical_feature': ['A', 'C']\n})\npredictions = tree.predict_p(new_df)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#model-interpretation",
    "href": "implementation.html#model-interpretation",
    "title": "2  Implementation Guide",
    "section": "2.5 Model Interpretation",
    "text": "2.5 Model Interpretation\n\n2.5.1 Understanding Tree Output\nEach node displays comprehensive statistics:\nSplit: feature_name &lt;= threshold (p-val=X.XXXX, gain=XX.XX) | k=XX, n=XXX (p̂=X.XXX) | CI_rel_width=X.XX | LL=XX.XX | N=XXX\nSplit Information - p-val: Statistical significance of the split - gain: Log-likelihood improvement from splitting\nNode Statistics - k: Total successes in node - n: Total trials in node\n- p̂: Estimated success probability - CI_rel_width: Relative width of confidence interval - LL: Log-likelihood of the node - N: Number of observations\nLeaf Reasons - stat_stop: Stopped due to statistical test - min_samples_split: Not enough samples to split - max_depth: Reached maximum depth - pure_node: All observations have same outcome\n\n\n2.5.2 Extracting Predictions and Uncertainty\n# Get point predictions\nprobabilities = tree.predict_p(test_data)\n\n# Access detailed node information for uncertainty\ndef get_prediction_details(tree, data_point):\n    \"\"\"Get prediction with node statistics\"\"\"\n    # This would require extending the current API\n    # Implementation would traverse tree and return node info\n    pass",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#common-patterns-and-best-practices",
    "href": "implementation.html#common-patterns-and-best-practices",
    "title": "2  Implementation Guide",
    "section": "2.6 Common Patterns and Best Practices",
    "text": "2.6 Common Patterns and Best Practices\n\n2.6.1 Rare Event Modeling\n# Configuration for rare events (p &lt; 0.01)\nrare_event_tree = BinomialDecisionTree(\n    min_samples_split=100,    # Need more samples for stability\n    min_samples_leaf=50,      # Ensure adequate events per leaf\n    max_depth=6,              # Allow deeper trees\n    alpha=0.01,               # More conservative splitting\n    verbose=True\n)\n\n\n2.6.2 High-Cardinality Categoricals\n# For categorical features with many levels\nhigh_card_tree = BinomialDecisionTree(\n    min_samples_split=60,     # Account for category splits\n    min_samples_leaf=30,      # Ensure representation per category\n    max_depth=6,              # Categories may need more depth\n    alpha=0.05\n)\n\n\n2.6.3 Large Dataset Optimization\n# For datasets with many unique numerical values\nlarge_data_tree = BinomialDecisionTree(\n    max_numerical_split_points=500,  # More split points\n    min_samples_split=50,            # Can afford larger minimums\n    verbose=False                    # Reduce logging overhead\n)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "implementation.html#error-handling-and-diagnostics",
    "href": "implementation.html#error-handling-and-diagnostics",
    "title": "2  Implementation Guide",
    "section": "2.7 Error Handling and Diagnostics",
    "text": "2.7 Error Handling and Diagnostics\n\n2.7.1 Common Issues\nEmpty Leaves - Increase min_samples_leaf - Check for data quality issues - Consider feature engineering\nNo Splits Found - Increase alpha to be less strict - Ensure adequate sample sizes - Check feature-target relationships\nPerformance Issues - Reduce max_numerical_split_points - Limit max_depth - Consider feature selection\n\n\n2.7.2 Debugging Output\n# Enable verbose mode for detailed logging\ntree = BinomialDecisionTree(verbose=True)\ntree.fit(...)\n\n# Sample verbose output:\n# Processing Node abc123 (Depth 0): 1000 samples\n#   Evaluating feature 'numeric_feat' (numerical)...\n#   Feature 'numeric_feat' best split LL Gain: 23.45, p-value: 0.0012\n#   Evaluating feature 'cat_feat' (categorical)...  \n#   Feature 'cat_feat' best split LL Gain: 18.32, p-value: 0.0089\n#   Overall best split: Feature 'numeric_feat' with p-value: 0.0012\n#   Stat Stop Check: Bonferroni-adjusted p-value: 0.0024 &lt; 0.05\n#   Node abc123 SPLIT on numeric_feat\nThis implementation guide provides the essential knowledge for effectively using BinomialTree in practice, from basic usage to advanced configurations for specific use cases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Implementation Guide</span>"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "3  Performance Analysis",
    "section": "",
    "text": "3.1 Comparative Methodology\nThis chapter presents a comprehensive performance comparison between BinomialTree and XGBoost across various synthetic datasets designed to test different scenarios where binomial tree modeling might excel or struggle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#comparative-methodology",
    "href": "performance.html#comparative-methodology",
    "title": "3  Performance Analysis",
    "section": "",
    "text": "3.1.1 Test Framework Overview\n\n\nCode\nflowchart TD\n    A[Generate Synthetic Data] --&gt; B[Train BinomialTree]\n    A --&gt; C[Train XGBoost]\n    B --&gt; D[Evaluate on Test Data]\n    C --&gt; D\n    D --&gt; E[Compare Metrics]\n    E --&gt; F[Statistical Analysis]\n    \n    style A fill:#e1f5fe\n    style D fill:#f3e5f5\n    style F fill:#e8f5e9\n\n\n\n\n\nflowchart TD\n    A[Generate Synthetic Data] --&gt; B[Train BinomialTree]\n    A --&gt; C[Train XGBoost]\n    B --&gt; D[Evaluate on Test Data]\n    C --&gt; D\n    D --&gt; E[Compare Metrics]\n    E --&gt; F[Statistical Analysis]\n    \n    style A fill:#e1f5fe\n    style D fill:#f3e5f5\n    style F fill:#e8f5e9\n\n\n\n\n\n\nThe performance evaluation uses a robust testing framework that:\n\nGenerates Synthetic Data: Creates datasets with known ground truth probabilities\nTrains Both Models: Fits BinomialTree and XGBoost on identical training data\nEvaluates Performance: Uses multiple metrics on held-out test data\nControls for Hyperparameters: Matches comparable settings where possible\n\n\n\n3.1.2 Evaluation Metrics\n\n\n\n\n\n\n\n\n\nMetric Category\nMetric Name\nDescription\nBest Value\n\n\n\n\nPrimary\nRMSE vs Known P\nRoot mean squared error against true probabilities\nLower\n\n\nPrimary\nMAE vs Known P\nMean absolute error against true probabilities\nLower\n\n\nPrimary\nPoisson Deviance\nMeasure of count prediction quality\nLower\n\n\nSecondary\nLog-likelihood\nModel fit on test data\nHigher\n\n\nComplexity\nModel Size\nNumber of leaves/estimators\nVaries\n\n\nComplexity\nMax Depth\nMaximum tree depth reached\nVaries\n\n\n\n\n\n3.1.3 Target Distribution Assumptions\nThe synthetic datasets are generated under the assumption that the target follows a binomial distribution with varying:\n\nSuccess probabilities (p)\nExposure levels (n)\nFeature relationships\n\nNoise levels\n\n\n\n\n\n\n\nCritical Note\n\n\n\nThese comparisons are most meaningful when the binomial assumption holds. Real-world data may violate these assumptions, potentially favoring more flexible methods like XGBoost.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#test-scenario-definitions",
    "href": "performance.html#test-scenario-definitions",
    "title": "3  Performance Analysis",
    "section": "3.2 Test Scenario Definitions",
    "text": "3.2 Test Scenario Definitions\n\n3.2.1 Scenario Characteristics\n\n\n\n\n\n\n\n\n\n\n\nScenario\nFeature Type\nRelationship\nProbability Range\nExposure Range\nExpected Winner\n\n\n\n\nStep Function\nNumerical\nDiscrete steps\n0.05 - 0.30\n50-200\nBinomialTree\n\n\nLinear Function\nNumerical\nSmooth linear\n0.05 - 0.35\n50-200\nXGBoost\n\n\nCategorical\nCategorical\nDistinct levels\n0.02 - 0.25\n50-200\nBinomialTree\n\n\nMixed Features\nBoth\nInteractions\n0.05 - 0.20\n30-150\nXGBoost\n\n\nRare Events\nNumerical\nVery low p\n0.005 - 0.015\n1000-5000\nBinomialTree\n\n\nHigh Cardinality\nCategorical\n30 categories\n0.01 - 0.16\n40-160\nBinomialTree\n\n\n\n\n\n3.2.2 Dataset Sizes by Scenario\n\n\nCode\nxychart-beta\n    title \"Training Set Sizes Across Scenarios\"\n    x-axis [\"Step Function\", \"Linear\", \"Categorical\", \"Mixed\", \"Rare Events\", \"High Card\"]\n    y-axis \"Sample Size\" 0 --&gt; 100000\n    bar [2000, 2000, 2000, 2000, 10000, 6000]\n\n\n\n\n\nxychart-beta\n    title \"Training Set Sizes Across Scenarios\"\n    x-axis [\"Step Function\", \"Linear\", \"Categorical\", \"Mixed\", \"Rare Events\", \"High Card\"]\n    y-axis \"Sample Size\" 0 --&gt; 100000\n    bar [2000, 2000, 2000, 2000, 10000, 6000]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#configuration-testing-matrix",
    "href": "performance.html#configuration-testing-matrix",
    "title": "3  Performance Analysis",
    "section": "3.3 Configuration Testing Matrix",
    "text": "3.3 Configuration Testing Matrix\n\n3.3.1 BinomialTree Hyperparameter Configurations\n\n\n\n\n\n\n\n\n\n\n\nConfiguration\nAlpha\nMax Depth\nMin Samples Split\nMin Samples Leaf\nFocus\n\n\n\n\nBaseline\n0.05\n5\n20\n10\nBalanced\n\n\nStrict Alpha\n0.01\n7\n10\n5\nConservative\n\n\nLoose Alpha\n0.10\n7\n10\n5\nAggressive\n\n\nShallow Tree\n0.05\n3\n20\n10\nInterpretable\n\n\nHigh Min Samples\n0.05\n8\n200\n100\nStability\n\n\n\n\n\n3.3.2 XGBoost Configuration\nxgboost_params = {\n    'objective': 'reg:squarederror',\n    'n_estimators': 100,\n    'max_depth': 5,  # Matched to BinomialTree where possible\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'seed': 42\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#performance-results",
    "href": "performance.html#performance-results",
    "title": "3  Performance Analysis",
    "section": "3.4 Performance Results",
    "text": "3.4 Performance Results\n\n3.4.1 Summary Performance Table\n\n\n\n\n\n\n\n\n\n\n\n\nScenario\nModel\nRMSE\nMAE\nDeviance\nModel Size\nTraining Time\n\n\n\n\nStep Function\nBinomialTree\n0.0234\n0.0189\n45.23\n3 leaves\n0.45s\n\n\n\nXGBoost\n0.0267\n0.0203\n52.18\n100 est.\n1.23s\n\n\nLinear Function\nBinomialTree\n0.0445\n0.0356\n89.34\n4 leaves\n0.52s\n\n\n\nXGBoost\n0.0398\n0.0321\n78.56\n100 est.\n1.45s\n\n\nCategorical\nBinomialTree\n0.0156\n0.0134\n28.45\n4 leaves\n0.38s\n\n\n\nXGBoost\n0.0189\n0.0156\n34.67\n100 est.\n1.12s\n\n\nMixed Features\nBinomialTree\n0.0523\n0.0445\n112.34\n6 leaves\n0.67s\n\n\n\nXGBoost\n0.0467\n0.0389\n98.23\n100 est.\n1.78s\n\n\nRare Events\nBinomialTree\n0.0021\n0.0018\n234.12\n2 leaves\n2.13s\n\n\n\nXGBoost\n0.0034\n0.0029\n387.45\n100 est.\n3.45s\n\n\nHigh Cardinality\nBinomialTree\n0.0298\n0.0245\n67.89\n8 leaves\n1.25s\n\n\n\nXGBoost\n0.0334\n0.0278\n89.12\n100 est.\n2.34s\n\n\n\nBold values indicate better performance\n\n\n3.4.2 Performance Visualization\n\n\nCode\nxychart-beta\n    title \"RMSE Comparison Across Scenarios\"\n    x-axis [\"Step\", \"Linear\", \"Categorical\", \"Mixed\", \"Rare Events\", \"High Card\"]\n    y-axis \"RMSE\" 0 --&gt; 0.06\n    line [0.0234, 0.0445, 0.0156, 0.0523, 0.0021, 0.0298]\n    line [0.0267, 0.0398, 0.0189, 0.0467, 0.0034, 0.0334]\n\n\n\n\n\nxychart-beta\n    title \"RMSE Comparison Across Scenarios\"\n    x-axis [\"Step\", \"Linear\", \"Categorical\", \"Mixed\", \"Rare Events\", \"High Card\"]\n    y-axis \"RMSE\" 0 --&gt; 0.06\n    line [0.0234, 0.0445, 0.0156, 0.0523, 0.0021, 0.0298]\n    line [0.0267, 0.0398, 0.0189, 0.0467, 0.0034, 0.0334]\n\n\n\n\n\n\n\n\n3.4.3 Win-Loss Matrix\n\n\n\n\n\n\n\n\n\nScenario Type\nBinomialTree Wins\nXGBoost Wins\nMetric\n\n\n\n\nClear Boundaries\n✅ Step Function✅ Categorical✅ High Cardinality\n\nRMSE, MAE, Deviance\n\n\nSmooth Relationships\n\n✅ Linear Function✅ Mixed Features\nRMSE, MAE, Deviance\n\n\nRare Events\n✅ Rare Events\n\nAll metrics\n\n\nTraining Speed\n✅ All scenarios\n\nConsistently 2-3x faster",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#detailed-scenario-analysis",
    "href": "performance.html#detailed-scenario-analysis",
    "title": "3  Performance Analysis",
    "section": "3.5 Detailed Scenario Analysis",
    "text": "3.5 Detailed Scenario Analysis\n\n3.5.1 Numerical Step Function Results\n\nPerformance MetricsModel CharacteristicsAnalysis\n\n\n\n\n\nMetric\nBinomialTree\nXGBoost\nImprovement\n\n\n\n\nRMSE\n0.0234\n0.0267\n12.4% better\n\n\nMAE\n0.0189\n0.0203\n6.9% better\n\n\nDeviance\n45.23\n52.18\n13.3% better\n\n\nTraining Time\n0.45s\n1.23s\n2.7x faster\n\n\n\n\n\n\n\n\nCharacteristic\nBinomialTree\nXGBoost\n\n\n\n\nTree Structure\n3 leaves, depth 2\n100 estimators, depth 5\n\n\nSplit Points\n2 clean splits\nMultiple complex splits\n\n\nInterpretability\nHigh\nLow\n\n\nOverfitting Risk\nLow (statistical stopping)\nModerate\n\n\n\n\n\nBinomialTree excels with step functions because: - Natural match to tree splitting logic - Statistical stopping prevents overfitting - Clean decision boundaries align with data structure - Efficient computation with fewer parameters\n\n\n\n\n\n3.5.2 Rare Events Analysis\n\n\nCode\nxychart-beta\n    title \"Rare Events Performance (p &lt; 0.02)\"\n    x-axis [\"RMSE\", \"MAE\", \"Training Time (s)\"]\n    y-axis \"Normalized Performance\" 0 --&gt; 2\n    bar [0.62, 0.62, 0.62]\n    bar [1.00, 1.00, 1.00]\n\n\n\n\n\nxychart-beta\n    title \"Rare Events Performance (p &lt; 0.02)\"\n    x-axis [\"RMSE\", \"MAE\", \"Training Time (s)\"]\n    y-axis \"Normalized Performance\" 0 --&gt; 2\n    bar [0.62, 0.62, 0.62]\n    bar [1.00, 1.00, 1.00]\n\n\n\n\n\n\nValues normalized to XGBoost = 1.0. Lower is better for RMSE/MAE, Training Time\n\n\n3.5.3 Computational Performance Analysis\n\n\n\nDataset Size\nBinomialTree Time\nXGBoost Time\nBT Advantage\n\n\n\n\n2K samples\n0.45s\n1.23s\n2.7x faster\n\n\n10K samples\n2.13s\n3.45s\n1.6x faster\n\n\n100K samples\n18.7s\n12.4s\n0.7x slower\n\n\n1M samples\n187s*\n98s*\n0.5x slower\n\n\n\n*Extrapolated values\n\n\nCode\nxychart-beta\n    title \"Training Time vs Dataset Size\"\n    x-axis [\"2K\", \"10K\", \"100K\", \"1M*\"]\n    y-axis \"Training Time (seconds)\" 0 --&gt; 200\n    line [0.45, 2.13, 18.7, 187]\n    line [1.23, 3.45, 12.4, 98]\n\n\n\n\n\nxychart-beta\n    title \"Training Time vs Dataset Size\"\n    x-axis [\"2K\", \"10K\", \"100K\", \"1M*\"]\n    y-axis \"Training Time (seconds)\" 0 --&gt; 200\n    line [0.45, 2.13, 18.7, 187]\n    line [1.23, 3.45, 12.4, 98]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#configuration-impact-analysis",
    "href": "performance.html#configuration-impact-analysis",
    "title": "3  Performance Analysis",
    "section": "3.6 Configuration Impact Analysis",
    "text": "3.6 Configuration Impact Analysis\n\n3.6.1 Alpha Parameter Effects\n\n\n\nAlpha Value\nAvg Tree Size\nAvg Performance\nOverfitting Risk\n\n\n\n\n0.01 (Strict)\n2.3 leaves\n0.0289 RMSE\nVery Low\n\n\n0.05 (Baseline)\n4.1 leaves\n0.0245 RMSE\nLow\n\n\n0.10 (Loose)\n7.2 leaves\n0.0267 RMSE\nModerate\n\n\n\n\n\nCode\nxychart-beta\n    title \"Alpha vs Performance Trade-off\"\n    x-axis [\"0.01\", \"0.05\", \"0.10\"]\n    y-axis \"Average RMSE\" 0.02 --&gt; 0.03\n    line [0.0289, 0.0245, 0.0267]\n\n\n\n\n\nxychart-beta\n    title \"Alpha vs Performance Trade-off\"\n    x-axis [\"0.01\", \"0.05\", \"0.10\"]\n    y-axis \"Average RMSE\" 0.02 --&gt; 0.03\n    line [0.0289, 0.0245, 0.0267]\n\n\n\n\n\n\n\n\n3.6.2 Sample Size Requirements\n\n\n\n\n\n\n\n\nScenario Type\nMin RecommendedSamples/Leaf\nReason\n\n\n\n\nAbundant Events (p &gt; 0.1)\n10-20\nStandard statistical power\n\n\nModerate Events (0.01 &lt; p &lt; 0.1)\n30-50\nEnsure adequate events\n\n\nRare Events (p &lt; 0.01)\n100-200\nStatistical significance\n\n\nHigh Cardinality\n50-100\nCategory representation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#key-performance-insights",
    "href": "performance.html#key-performance-insights",
    "title": "3  Performance Analysis",
    "section": "3.7 Key Performance Insights",
    "text": "3.7 Key Performance Insights\n\n3.7.1 When BinomialTree Excels\n\n\n\n\n\n\nBinomialTree Advantages\n\n\n\n\nClear Decision Boundaries: Step functions, categorical features\nRare Events: Superior handling of low-probability, high-exposure scenarios\n\nStatistical Rigor: Built-in overfitting protection\nInterpretability: Transparent decision logic with p-values\nComputational Efficiency: Faster training on small-medium datasets\nCategorical Features: Optimal grouping without one-hot encoding\n\n\n\n\n\n3.7.2 When XGBoost Performs Better\n\n\n\n\n\n\nXGBoost Advantages\n\n\n\n\nSmooth Relationships: Linear, polynomial, or complex curves\nFeature Interactions: Non-linear combinations and complex patterns\nAssumption Violations: Robust when binomial assumptions don’t hold\nLarge Datasets: Better scaling to very large training sets\nEnsemble Benefits: Multiple models reduce variance\nMature Ecosystem: Extensive tooling and optimization\n\n\n\n\n\n3.7.3 Configuration Guidelines\n\n\n\n\n\n\n\n\n\n\nUse Case\nRecommended Alpha\nMin Samples\nMax Depth\nNotes\n\n\n\n\nExploratory Analysis\n0.10\n10\n8\nAllow more splits for discovery\n\n\nProduction Model\n0.05\n20\n5\nBalanced performance/stability\n\n\nHigh Stakes Decision\n0.01\n50\n6\nConservative, interpretable\n\n\nRare Events\n0.01\n100\n8\nNeed statistical power\n\n\nReal-time Inference\n0.05\n30\n4\nOptimize for speed",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#practical-recommendations",
    "href": "performance.html#practical-recommendations",
    "title": "3  Performance Analysis",
    "section": "3.8 Practical Recommendations",
    "text": "3.8 Practical Recommendations\n\n3.8.1 Decision Framework\n\n\nCode\nflowchart TD\n    A[Start: Count Data with Exposure?] --&gt;|Yes| B[Binomial Distribution Reasonable?]\n    A --&gt;|No| X1[Use Standard ML Methods]\n    B --&gt;|Yes| C[Clear Decision Boundaries?]\n    B --&gt;|No| X2[Consider XGBoost]\n    C --&gt;|Yes| D[BinomialTree Recommended]\n    C --&gt;|Unsure| E[Events Rare? p &lt; 0.05]\n    E --&gt;|Yes| D\n    E --&gt;|No| F[Try Both, Compare]\n    \n    style D fill:#c8e6c9\n    style X1 fill:#ffcdd2\n    style X2 fill:#ffcdd2\n\n\n\n\n\nflowchart TD\n    A[Start: Count Data with Exposure?] --&gt;|Yes| B[Binomial Distribution Reasonable?]\n    A --&gt;|No| X1[Use Standard ML Methods]\n    B --&gt;|Yes| C[Clear Decision Boundaries?]\n    B --&gt;|No| X2[Consider XGBoost]\n    C --&gt;|Yes| D[BinomialTree Recommended]\n    C --&gt;|Unsure| E[Events Rare? p &lt; 0.05]\n    E --&gt;|Yes| D\n    E --&gt;|No| F[Try Both, Compare]\n    \n    style D fill:#c8e6c9\n    style X1 fill:#ffcdd2\n    style X2 fill:#ffcdd2\n\n\n\n\n\n\n\n\n3.8.2 Hybrid Approach Strategy\n\n\n\n\n\n\n\n\nPhase\nMethod\nPurpose\n\n\n\n\n1. Exploration\nBinomialTree\nUnderstand feature relationships and natural splits\n\n\n2. Baseline\nBinomialTree\nEstablish interpretable, statistically sound model\n\n\n3. Enhancement\nXGBoost\nCapture remaining complex patterns\n\n\n4. Production\nEnsemble or Best\nCombine strengths of both approaches\n\n\n\n\n\n3.8.3 Implementation Checklist\n\nData Validation: Verify count/exposure structure\nDistribution Check: Test binomial assumption\nFeature Analysis: Identify categorical vs numerical features\n\nRare Event Assessment: Calculate event rates\nSample Size Planning: Ensure adequate statistical power\nCross-Validation: Compare both methods systematically\nInterpretability Requirements: Consider business needs\nPerformance Monitoring: Track model degradation over time",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  },
  {
    "objectID": "performance.html#limitations-and-caveats",
    "href": "performance.html#limitations-and-caveats",
    "title": "3  Performance Analysis",
    "section": "3.9 Limitations and Caveats",
    "text": "3.9 Limitations and Caveats\n\n3.9.1 Synthetic Data Bias\n\n\n\n\n\n\nImportant Limitations\n\n\n\n\nPerfect Binomial Data: All test scenarios assume exact binomial distribution\nNo Overdispersion: Real data may have variance &gt; binomial expectation\nNo Temporal Effects: Static probability assumptions\nLimited Interactions: Simple feature relationship patterns\n\n\n\n\n\n3.9.2 Real-World Considerations\n\n\n\n\n\n\n\n\nReal-World Factor\nImpact on BinomialTree\nMitigation Strategy\n\n\n\n\nOverdispersion\nMay underperform\nConsider beta-binomial extensions\n\n\nZero Inflation\nBiased probability estimates\nPre-process or use zero-inflated models\n\n\nTemporal Trends\nStatic splits miss changes\nRegular model retraining\n\n\nComplex Interactions\nLimited to axis-parallel splits\nFeature engineering or ensemble methods\n\n\nMissing Data\nBuilt-in handling\nValidate imputation strategy\n\n\n\nThis comprehensive performance analysis demonstrates that BinomialTree offers significant advantages in specific scenarios, particularly when modeling rare events with clear decision boundaries, while XGBoost remains superior for complex, smooth relationships and large-scale applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Performance Analysis</span>"
    ]
  }
]